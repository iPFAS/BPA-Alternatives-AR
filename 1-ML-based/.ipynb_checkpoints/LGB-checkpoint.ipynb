{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5879c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, SelectFromModel\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_recall_curve, auc, mean_squared_error, \\\n",
    "    r2_score, mean_absolute_error,cohen_kappa_score,accuracy_score,f1_score,matthews_corrcoef,precision_score,recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "start = time.time()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def standardize(col):\n",
    "    return (col - np.mean(col)) / np.std(col)\n",
    "\n",
    "# the metrics for classification\n",
    "def statistical(y_true, y_pred, y_pro):\n",
    "    c_mat = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = list(c_mat.flatten())\n",
    "    se = tp / (tp + fn)\n",
    "    sp = tn / (tn + fp)\n",
    "    auc_prc = auc(precision_recall_curve(y_true, y_pro, pos_label=1)[1],\n",
    "                  precision_recall_curve(y_true, y_pro, pos_label=1)[0])\n",
    "    acc = (tp + tn) / (tn + fp + fn + tp)\n",
    "#     acc_skl = accuracy_score(y_true, y_pred)\n",
    "    auc_roc = roc_auc_score(y_true, y_pro)\n",
    "    recall = se\n",
    "#     recall_skl = recall_score(y_true, y_pred)\n",
    "    precision = tp / (tp + fp)\n",
    "#     precision_skl = precision_score(y_true, y_pred)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "#     f1_skl = f1_score(y_true, y_pred)\n",
    "    kappa = cohen_kappa_score(y_true,y_pred)\n",
    "    mcc = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn) + 1e-8)\n",
    "#     mcc_skl = matthews_corrcoef(y_true,y_pred)\n",
    "    return tn,fp,fn,tp,se,sp,auc_prc,acc,auc_roc,recall,precision,f1,kappa,mcc\n",
    "\n",
    "def all_one_zeros(data):\n",
    "    if (len(np.unique(data)) == 2):\n",
    "        flag = False\n",
    "    else:\n",
    "        flag = True\n",
    "    return flag\n",
    "\n",
    "\n",
    "feature_selection = False\n",
    "tasks_dic = {'1-AR-Alva-6108-slim-Normalize-group.csv': ['activity']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49531695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'actual_estimator__num_leaves': IntUniformDistribution(high=256, low=2, step=1),\n",
    "#  'actual_estimator__learning_rate': LogUniformDistribution(high=0.5, low=1e-06),\n",
    "#  'actual_estimator__n_estimators': IntUniformDistribution(high=300, low=10, step=1),\n",
    "#  'actual_estimator__min_split_gain': UniformDistribution(high=1.0, low=0.0),\n",
    "#  'actual_estimator__reg_alpha': LogUniformDistribution(high=10.0, low=1e-10),\n",
    "#  'actual_estimator__reg_lambda': LogUniformDistribution(high=10.0, low=1e-10),\n",
    "#  'actual_estimator__feature_fraction': UniformDistribution(high=1.0, low=0.4),\n",
    "#  'actual_estimator__bagging_fraction': UniformDistribution(high=1.0, low=0.4),\n",
    "#  'actual_estimator__bagging_freq': IntUniformDistribution(high=7, low=0, step=1),\n",
    "#  'actual_estimator__min_child_samples': IntUniformDistribution(high=100, low=1, step=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6daf58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '1-AR-Alva-6108-slim-Normalize-group.csv' \n",
    "task_type = 'cla'  # 'reg' or 'cla'\n",
    "dataset_label = file_name.split('/')[-1].split('_')[0]\n",
    "tasks = tasks_dic[dataset_label]\n",
    "OPT_ITERS = 50\n",
    "repetitions = 10\n",
    "num_pools = 10\n",
    "unbalance = True\n",
    "patience = 100\n",
    "ecfp = True\n",
    "space_ = {'num_leaves': hp.choice('num_leaves', range(2,256,1)),\n",
    "          'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "          'n_estimators': hp.choice('n_estimators', range(10,300,1)),\n",
    "          'min_split_gain': hp.uniform('min_split_gain', 0., 1),\n",
    "          'reg_alpha': hp.loguniform('reg_alpha', 1e-10, 10.0),\n",
    "          'reg_lambda': hp.loguniform('reg_lambda', 1e-10, 10.0),\n",
    "          'feature_fraction': hp.uniform('feature_fraction', 0.4, 1),\n",
    "          'bagging_fraction': hp.uniform('bagging_fraction', 0.4, 1),\n",
    "          'bagging_freq': hp.choice('bagging_freq', range(0,7,1)),\n",
    "          'min_child_samples': hp.choice('min_child_samples', range(1,100,1))\n",
    "          }\n",
    "num_leaves_ls = range(2,256,1)\n",
    "n_estimators_ls = range(10,300,1)\n",
    "bagging_freq_ls = range(0,7,1)\n",
    "dataset = pd.read_csv(file_name)\n",
    "pd_res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5508043",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the num of retained features for the 1-AR-Alva-6108-slim-Normalize-group.csv activity is: 1508\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.951913451882592, subsample=1.0 will be ignored. Current value: bagging_fraction=0.951913451882592\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6213922442158696, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6213922442158696\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7020387076612011, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7020387076612011\n",
      "[LightGBM] [Warning] feature_fraction is set=0.963429072313533, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.963429072313533\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9695954970003188, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9695954970003188\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8460162061613135, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8460162061613135\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7784916689804147, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7784916689804147\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4073301455053756, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4073301455053756\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9373992433793206, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9373992433793206\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45178159061136713, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45178159061136713\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7890359295173249, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7890359295173249\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9607897780775397, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9607897780775397\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6942275382596246, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6942275382596246\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45103301407994223, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45103301407994223\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7175584557884982, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7175584557884982\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6106424873172064, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6106424873172064\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9320730504037458, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9320730504037458\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8389894215944265, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8389894215944265\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.430833167800964, subsample=1.0 will be ignored. Current value: bagging_fraction=0.430833167800964\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5767509422900097, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5767509422900097\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6947115883758691, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6947115883758691\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7464278528244941, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7464278528244941\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4999855772586039, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4999855772586039\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6655960809424587, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6655960809424587\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.43924397962019945, subsample=1.0 will be ignored. Current value: bagging_fraction=0.43924397962019945\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4205320054667316, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4205320054667316\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.984564011189932, subsample=1.0 will be ignored. Current value: bagging_fraction=0.984564011189932\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6075089398819011, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6075089398819011\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6913380855094806, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6913380855094806\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5706998419105604, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5706998419105604\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8393424316756911, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8393424316756911\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5172682525397253, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5172682525397253\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4563492997906699, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4563492997906699\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6966278679557387, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6966278679557387\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7331219420090291, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7331219420090291\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6218449647085951, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6218449647085951\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4126711023992311, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4126711023992311\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7331990994065376, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7331990994065376\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4133989287744276, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4133989287744276\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6131427621751556, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6131427621751556\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.5911712939899493, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5911712939899493\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5028241613156662, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5028241613156662\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5825502507084752, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5825502507084752\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5280318776756985, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5280318776756985\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5825346682438898, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5825346682438898\n",
      "[LightGBM] [Warning] feature_fraction is set=0.540534118838398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.540534118838398\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6055165799467639, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6055165799467639\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6643456582827278, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6643456582827278\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5358476686099356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5358476686099356\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7822070200034072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7822070200034072\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5207351497662895, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5207351497662895\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8249900884052536, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8249900884052536\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.856763353734341, subsample=1.0 will be ignored. Current value: bagging_fraction=0.856763353734341\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7870800610504398, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7870800610504398\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6397323281888985, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6397323281888985\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9115508182822618, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9115508182822618\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5242652711611565, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5242652711611565\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7634835487907506, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7634835487907506\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7569866659919019, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7569866659919019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8753233433431789, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8753233433431789\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.637703644146348, subsample=1.0 will be ignored. Current value: bagging_fraction=0.637703644146348\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6777130934629608, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6777130934629608\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8855520343624466, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8855520343624466\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7970645028793445, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7970645028793445\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7325588715431418, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7325588715431418\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9378614016744644, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9378614016744644\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8091878806787741, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8091878806787741\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7169696136706858, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7169696136706858\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6474553438238025, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6474553438238025\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8984990368284863, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8984990368284863\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.47576357974959615, subsample=1.0 will be ignored. Current value: bagging_fraction=0.47576357974959615\n",
      "[LightGBM] [Warning] feature_fraction is set=0.987202738048057, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.987202738048057\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5435020211594354, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5435020211594354\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6405252475172527, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6405252475172527\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7427040364386672, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7427040364386672\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7952839763961901, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7952839763961901\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6676429513590919, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6676429513590919\n",
      "[LightGBM] [Warning] feature_fraction is set=0.486875671469276, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.486875671469276\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9044927567950738, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9044927567950738\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8623153006899666, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8623153006899666\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7641778480028426, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7641778480028426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.5751511788757719, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5751511788757719\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8163440483846663, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8163440483846663\n",
      "[LightGBM] [Warning] feature_fraction is set=0.46880438334216235, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.46880438334216235\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5547887225918672, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5547887225918672\n",
      "[LightGBM] [Warning] feature_fraction is set=0.763017125968665, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.763017125968665\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.47643550886767955, subsample=1.0 will be ignored. Current value: bagging_fraction=0.47643550886767955\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6416130199590551, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6416130199590551\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6201656583961093, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6201656583961093\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8183143295329568, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8183143295329568\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7211414008352154, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7211414008352154\n",
      "[LightGBM] [Warning] feature_fraction is set=0.698862194949191, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.698862194949191\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6749255266214942, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6749255266214942\n",
      "[LightGBM] [Warning] feature_fraction is set=0.41254510828160224, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.41254510828160224\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7931421696661788, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7931421696661788\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5578533596702133, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5578533596702133\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.955873932222928, subsample=1.0 will be ignored. Current value: bagging_fraction=0.955873932222928\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6422377769846723, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6422377769846723\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8533783534982557, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8533783534982557\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7282569361017286, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7282569361017286\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "the best hyper-parameters for 1-AR-Alva-6108-slim-Normalize-group.csv activity are:   {'bagging_fraction': 0.5358476686099356, 'bagging_freq': 2, 'feature_fraction': 0.7822070200034072, 'learning_rate': 0.14340555482403722, 'min_child_samples': 78, 'min_split_gain': 0.09176572492525548, 'n_estimators': 179, 'num_leaves': 246, 'reg_alpha': 2.787999743315647, 'reg_lambda': 2.8994201110069877}\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5358476686099356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5358476686099356\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7822070200034072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7822070200034072\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "train 0.9999591805360073 0.9998287882421338\n",
      "valid 0.9491278758694489 0.859846352748534\n",
      "test 0.9328243492518958 0.8518364728745357\n"
     ]
    }
   ],
   "source": [
    "def hyper_runing(subtask):\n",
    "    cols = [subtask]\n",
    "    cols.extend(dataset.columns[(len(tasks) + 1):])\n",
    "    sub_dataset = dataset[cols]\n",
    "\n",
    "    # detect the na in the subtask (y cloumn)\n",
    "    rm_index = sub_dataset[subtask][sub_dataset[subtask].isnull()].index\n",
    "    sub_dataset.drop(index=rm_index, inplace=True)\n",
    "\n",
    "    # remove the features with na\n",
    "    sub_dataset = sub_dataset.dropna(axis=1)\n",
    "    # *******************\n",
    "    # demension reduction\n",
    "    # *******************\n",
    "    # Removing features with low variance\n",
    "    # threshold = 0.05\n",
    "    data_fea_var = sub_dataset.iloc[:, 2:].var()\n",
    "    del_fea1 = list(data_fea_var[data_fea_var <= 0.05].index)\n",
    "    sub_dataset.drop(columns=del_fea1, inplace=True)\n",
    "\n",
    "    # pair correlations\n",
    "    # threshold = 0.95\n",
    "    data_fea_corr = sub_dataset.iloc[:, 2:].corr()\n",
    "    del_fea2_col = []\n",
    "    del_fea2_ind = []\n",
    "    length = data_fea_corr.shape[1]\n",
    "    for i in range(length):\n",
    "        for j in range(i + 1, length):\n",
    "            if abs(data_fea_corr.iloc[i, j]) >= 0.95:\n",
    "                del_fea2_col.append(data_fea_corr.columns[i])\n",
    "                del_fea2_ind.append(data_fea_corr.index[j])\n",
    "    sub_dataset.drop(columns=del_fea2_ind, inplace=True)\n",
    "\n",
    "    # standardize the features\n",
    "    cols_ = list(sub_dataset.columns)[2:]\n",
    "    if not ecfp :\n",
    "        sub_dataset[cols_] = sub_dataset[cols_].apply(standardize, axis=0)\n",
    "\n",
    "    # get the attentivefp data splits\n",
    "    data_tr = sub_dataset[sub_dataset['group'] == 'train']\n",
    "    data_va = sub_dataset[sub_dataset['group'] == 'valid']\n",
    "    data_te = sub_dataset[sub_dataset['group'] == 'test']\n",
    "\n",
    "    # prepare data for training\n",
    "    # training set\n",
    "    data_tr_y = data_tr[subtask].values.reshape(-1, 1)\n",
    "    data_tr_x = np.array(data_tr.iloc[:, 2:].values)\n",
    "\n",
    "    # validation set\n",
    "    data_va_y = data_va[subtask].values.reshape(-1, 1)\n",
    "    data_va_x = np.array(data_va.iloc[:, 2:].values)\n",
    "\n",
    "    # test set\n",
    "    data_te_y = data_te[subtask].values.reshape(-1, 1)\n",
    "    data_te_x = np.array(data_te.iloc[:, 2:].values)\n",
    "\n",
    "    if feature_selection:\n",
    "        # univariate feature selection\n",
    "        trans1 = SelectPercentile(f_classif, percentile=80)\n",
    "        trans1.fit(data_tr_x, data_tr_y)\n",
    "        data_tr_x = trans1.transform(data_tr_x)\n",
    "        data_va_x = trans1.transform(data_va_x)\n",
    "        data_te_x = trans1.transform(data_te_x)\n",
    "\n",
    "        # select from model\n",
    "        clf = LGBMClassifier(n_jobs=12, random_state=1)\n",
    "        clf = clf.fit(data_tr_x, data_tr_y)\n",
    "        trans2 = SelectFromModel(clf, prefit=True)\n",
    "\n",
    "        data_tr_x = trans2.transform(data_tr_x)\n",
    "        data_va_x = trans2.transform(data_va_x)\n",
    "        data_te_x = trans2.transform(data_te_x)\n",
    "\n",
    "    num_fea = data_tr_x.shape[1]\n",
    "    print('the num of retained features for the ' + dataset_label + ' ' + subtask + ' is:', num_fea)\n",
    "\n",
    "    def hyper_opt(args):\n",
    "        model = LGBMClassifier(**args, n_jobs=-1, random_state=1) if task_type == 'cla' else LGBMRegressor(**args, n_jobs=6,\n",
    "                                                                                                   random_state=1)\n",
    "\n",
    "        model.fit(data_tr_x, data_tr_y, eval_metric='auc' if task_type == 'cla' else 'rmse',\n",
    "                  eval_set=[(data_va_x, data_va_y)],\n",
    "                  early_stopping_rounds=patience, verbose=False)\n",
    "        val_preds = model.predict_proba(data_va_x) if task_type == 'cla' else \\\n",
    "            model.predict(data_va_x)\n",
    "        loss = 1 - roc_auc_score(data_va_y, val_preds[:, 1]) if task_type == 'cla' else np.sqrt(\n",
    "            mean_squared_error(data_va_y, val_preds))\n",
    "        return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "    # start hyper-parameters optimization\n",
    "    trials = Trials()\n",
    "    best_results = fmin(hyper_opt, space_, algo=tpe.suggest, max_evals=OPT_ITERS, trials=trials, show_progressbar=False)\n",
    "    print('the best hyper-parameters for ' + dataset_label + ' ' + subtask + ' are:  ', best_results)\n",
    "\n",
    "    best_model = LGBMClassifier(num_leaves=num_leaves_ls[best_results['num_leaves']],\n",
    "                                learning_rate=best_results['learning_rate'],\n",
    "                                n_estimators=n_estimators_ls[best_results['n_estimators']],\n",
    "                                min_split_gain=best_results['min_split_gain'],\n",
    "                                reg_alpha=best_results['reg_alpha'],\n",
    "                                reg_lambda=best_results['reg_lambda'],\n",
    "                                feature_fraction=best_results['feature_fraction'],\n",
    "                                bagging_fraction=best_results['bagging_fraction'],\n",
    "                                bagging_freq=bagging_freq_ls[best_results['bagging_freq']],\n",
    "                                min_child_samples=best_results['min_child_samples'],\n",
    "                                n_jobs=-1, random_state=1) \\\n",
    "        if task_type == 'cla' else LGBMRegressor(\n",
    "        num_leaves=num_leaves_ls[best_results['num_leaves']],\n",
    "                                        learning_rate=best_results['learning_rate'],\n",
    "                                        n_estimators=n_estimators_ls[best_results['n_estimators']],\n",
    "                                        min_split_gain=best_results['min_split_gain'],\n",
    "                                        reg_alpha=best_results['reg_alpha'],\n",
    "                                        reg_lambda=best_results['reg_lambda'],\n",
    "                                        feature_fraction=best_results['feature_fraction'],\n",
    "                                        bagging_fraction=best_results['bagging_fraction'],\n",
    "                                        bagging_freq=bagging_freq_ls[best_results['bagging_freq']],\n",
    "                                        min_child_samples=best_results['min_child_samples'],\n",
    "                                        n_jobs=-1, random_state=1, verbose=-1)  \n",
    "    \n",
    "    best_model.fit(data_tr_x, data_tr_y, eval_metric='auc' if task_type == 'cla' else 'rmse',\n",
    "                   eval_set=[(data_va_x, data_va_y)],\n",
    "                   early_stopping_rounds=patience, verbose=False)\n",
    "    num_of_compounds = len(sub_dataset)\n",
    "\n",
    "    if task_type == 'cla':\n",
    "        # training set\n",
    "        tr_pred = best_model.predict_proba(data_tr_x)\n",
    "        tr_results = [dataset_label, subtask, 'tr', num_fea, num_of_compounds, data_tr_y[data_tr_y == 1].shape[0],\n",
    "                      data_tr_y[data_tr_y == 0].shape[0],\n",
    "                      data_tr_y[data_tr_y == 0].shape[0] / data_tr_y[data_tr_y == 1].shape[0],\n",
    "                      num_leaves_ls[best_results['num_leaves']],\n",
    "                      best_results['learning_rate'],\n",
    "                      n_estimators_ls[best_results['n_estimators']],\n",
    "                      best_results['min_split_gain'],\n",
    "                      best_results['reg_alpha'],\n",
    "                      best_results['reg_lambda'],\n",
    "                      best_results['feature_fraction'],\n",
    "                      best_results['bagging_fraction'],\n",
    "                      bagging_freq_ls[best_results['bagging_freq']],\n",
    "                      best_results['min_child_samples']\n",
    "                      ]\n",
    "        tr_results.extend(statistical(data_tr_y, np.argmax(tr_pred, axis=1), tr_pred[:, 1]))\n",
    "        # validation set\n",
    "        va_pred = best_model.predict_proba(data_va_x)\n",
    "                      \n",
    "        va_results = [dataset_label, subtask, 'va', num_fea, num_of_compounds, data_va_y[data_va_y == 1].shape[0],\n",
    "                      data_va_y[data_va_y == 0].shape[0],\n",
    "                      data_va_y[data_va_y == 0].shape[0] / data_va_y[data_va_y == 1].shape[0],\n",
    "                      num_leaves_ls[best_results['num_leaves']],\n",
    "                      best_results['learning_rate'],\n",
    "                      n_estimators_ls[best_results['n_estimators']],\n",
    "                      best_results['min_split_gain'],\n",
    "                      best_results['reg_alpha'],\n",
    "                      best_results['reg_lambda'],\n",
    "                      best_results['feature_fraction'],\n",
    "                      best_results['bagging_fraction'],\n",
    "                      bagging_freq_ls[best_results['bagging_freq']],\n",
    "                      best_results['min_child_samples']]\n",
    "        va_results.extend(statistical(data_va_y, np.argmax(va_pred, axis=1), va_pred[:, 1]))\n",
    "\n",
    "        # test set\n",
    "        te_pred = best_model.predict_proba(data_te_x)\n",
    "        te_results = [dataset_label, subtask, 'te', num_fea, num_of_compounds, data_te_y[data_te_y == 1].shape[0],\n",
    "                      data_te_y[data_te_y == 0].shape[0],\n",
    "                      data_te_y[data_te_y == 0].shape[0] / data_te_y[data_te_y == 1].shape[0],\n",
    "                      num_leaves_ls[best_results['num_leaves']],\n",
    "                      best_results['learning_rate'],\n",
    "                      n_estimators_ls[best_results['n_estimators']],\n",
    "                      best_results['min_split_gain'],\n",
    "                      best_results['reg_alpha'],\n",
    "                      best_results['reg_lambda'],\n",
    "                      best_results['feature_fraction'],\n",
    "                      best_results['bagging_fraction'],\n",
    "                      bagging_freq_ls[best_results['bagging_freq']],\n",
    "                      best_results['min_child_samples']]\n",
    "        te_results.extend(statistical(data_te_y, np.argmax(te_pred, axis=1), te_pred[:, 1]))\n",
    "    else:\n",
    "        # training set\n",
    "        tr_pred = best_model.predict(data_tr_x)\n",
    "        tr_results = [dataset_label, subtask, 'tr', num_fea, num_of_compounds,\n",
    "                      num_leaves_ls[best_results['num_leaves']],\n",
    "                      best_results['learning_rate'],\n",
    "                      n_estimators_ls[best_results['n_estimators']],\n",
    "                      best_results['min_split_gain'],\n",
    "                      best_results['reg_alpha'],\n",
    "                      best_results['reg_lambda'],\n",
    "                      best_results['feature_fraction'],\n",
    "                      best_results['bagging_fraction'],\n",
    "                      bagging_freq_ls[best_results['bagging_freq']],\n",
    "                      best_results['min_child_samples'],\n",
    "                      np.sqrt(mean_squared_error(data_tr_y, tr_pred)), r2_score(data_tr_y, tr_pred),\n",
    "                      mean_absolute_error(data_tr_y, tr_pred)]\n",
    "\n",
    "        # validation set\n",
    "        va_pred = best_model.predict(data_va_x)\n",
    "        va_results = [dataset_label, subtask, 'va', num_fea, num_of_compounds,\n",
    "                      num_leaves_ls[best_results['num_leaves']],\n",
    "                      best_results['learning_rate'],\n",
    "                      n_estimators_ls[best_results['n_estimators']],\n",
    "                      best_results['min_split_gain'],\n",
    "                      best_results['reg_alpha'],\n",
    "                      best_results['reg_lambda'],\n",
    "                      best_results['feature_fraction'],\n",
    "                      best_results['bagging_fraction'],\n",
    "                      bagging_freq_ls[best_results['bagging_freq']],\n",
    "                      best_results['min_child_samples'],\n",
    "                      np.sqrt(mean_squared_error(data_va_y, va_pred)), r2_score(data_va_y, va_pred),\n",
    "                      mean_absolute_error(data_va_y, va_pred)]\n",
    "\n",
    "        # test set\n",
    "        te_pred = best_model.predict(data_te_x)\n",
    "        te_results = [dataset_label, subtask, 'te', num_fea, num_of_compounds,\n",
    "                      num_leaves_ls[best_results['num_leaves']],\n",
    "                      best_results['learning_rate'],\n",
    "                      n_estimators_ls[best_results['n_estimators']],\n",
    "                      best_results['min_split_gain'],\n",
    "                      best_results['reg_alpha'],\n",
    "                      best_results['reg_lambda'],\n",
    "                      best_results['feature_fraction'],\n",
    "                      best_results['bagging_fraction'],\n",
    "                      bagging_freq_ls[best_results['bagging_freq']],\n",
    "                      best_results['min_child_samples'],\n",
    "                      np.sqrt(mean_squared_error(data_te_y, te_pred)), r2_score(data_te_y, te_pred),\n",
    "                      mean_absolute_error(data_te_y, te_pred)]\n",
    "    return tr_results, va_results, te_results\n",
    "\n",
    "\n",
    "pool = multiprocessing.Pool(num_pools)\n",
    "res = pool.starmap(hyper_runing, zip(tasks))\n",
    "pool.close()\n",
    "pool.join()\n",
    "for item in res:\n",
    "    for i in range(3):\n",
    "        pd_res.append(item[i])\n",
    "if task_type == 'cla':                    \n",
    "    best_hyper = pd.DataFrame(pd_res, columns=['dataset', 'subtask', 'set',\n",
    "                                               'num_of_retained_feature',\n",
    "                                               'num_of_compounds', 'postives',\n",
    "                                               'negtives', 'negtives/postives',\n",
    "                                               'num_leaves', \n",
    "                                               'learning_rate','n_estimators','min_split_gain','reg_alpha',\n",
    "                                               'reg_lambda','feature_fraction','bagging_fraction','bagging_freq','min_child_samples',\n",
    "                                               'tn', 'fp', 'fn', 'tp', 'se', 'sp',\n",
    "                                               'auc_prc', 'acc', 'auc_roc','recall','precision','f1','kappa','mcc'])\n",
    "else:\n",
    "    best_hyper = pd.DataFrame(pd_res, columns=['dataset', 'subtask', 'set',\n",
    "                                               'num_leaves', \n",
    "                                               'learning_rate','n_estimators','min_split_gain','reg_alpha',\n",
    "                                               'reg_lambda','feature_fraction','bagging_fraction','bagging_freq','min_child_samples',\n",
    "                                               'rmse', 'r2', 'mae'])\n",
    "best_hyper.to_csv('./model/' + dataset_label + '_LGB_hyperopt_info.csv', index=0)\n",
    "\n",
    "if task_type == 'cla':\n",
    "    print('train', best_hyper[best_hyper['set'] == 'tr']['auc_roc'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'tr']['auc_prc'].mean())\n",
    "    print('valid', best_hyper[best_hyper['set'] == 'va']['auc_roc'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'va']['auc_prc'].mean())\n",
    "    print('test', best_hyper[best_hyper['set'] == 'te']['auc_roc'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'te']['auc_prc'].mean())\n",
    "else:\n",
    "    print('train', best_hyper[best_hyper['set'] == 'tr']['rmse'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'tr']['r2'].mean(), best_hyper[best_hyper['set'] == 'tr']['mae'].mean())\n",
    "    print('valid', best_hyper[best_hyper['set'] == 'va']['rmse'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'va']['r2'].mean(), best_hyper[best_hyper['set'] == 'va']['mae'].mean())\n",
    "    print('test', best_hyper[best_hyper['set'] == 'te']['rmse'].mean(),\n",
    "          best_hyper[best_hyper['set'] == 'te']['r2'].mean(), best_hyper[best_hyper['set'] == 'te']['mae'].mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67e5b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 repetitions based on thr best hypers\n",
    "dataset.drop(columns=['group'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49892864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed used in repetition 2 is 2\n",
      "random seed used in repetition 5 is 5\n",
      "random seed used in repetition 1 is 1\n",
      "random seed used in repetition 8 is 8\n",
      "random seed used in repetition 6 is 6\n",
      "random seed used in repetition 9 is 9\n",
      "random seed used in repetition 4 is 4\n",
      "random seed used in repetition 3 is 3\n",
      "random seed used in repetition 7 is 7\n",
      "random seed used in repetition 10 is 10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5358476686099356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5358476686099356\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7822070200034072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7822070200034072\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5358476686099356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5358476686099356\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7822070200034072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7822070200034072\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5358476686099356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5358476686099356\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5358476686099356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5358476686099356\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7822070200034072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7822070200034072\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7822070200034072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7822070200034072\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5358476686099356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5358476686099356\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5358476686099356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5358476686099356\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5358476686099356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5358476686099356\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5358476686099356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5358476686099356\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5358476686099356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5358476686099356\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5358476686099356, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5358476686099356\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7822070200034072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7822070200034072\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7822070200034072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7822070200034072\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7822070200034072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7822070200034072\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7822070200034072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7822070200034072\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7822070200034072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7822070200034072\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7822070200034072, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7822070200034072\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "1-AR-Alva-6108-slim-Normalize-group.csv_LGB: the mean auc_roc for the training set is 0.993 with std 0.008\n",
      "1-AR-Alva-6108-slim-Normalize-group.csv_LGB: the mean auc_roc for the validation set is 0.939 with std 0.011\n",
      "1-AR-Alva-6108-slim-Normalize-group.csv_LGB: the mean auc_roc for the test set is 0.935 with std 0.005\n"
     ]
    }
   ],
   "source": [
    "pd_res = []\n",
    "def best_model_runing(split):\n",
    "    seed = split\n",
    "    if task_type == 'cla':\n",
    "        while True:\n",
    "            training_data, data_te = train_test_split(sub_dataset, test_size=0.1, random_state=seed)\n",
    "            # the training set was further splited into the training set and validation set\n",
    "            data_tr, data_va = train_test_split(training_data, test_size=0.1, random_state=seed)\n",
    "            if (all_one_zeros(data_tr[subtask]) or all_one_zeros(data_va[subtask]) or all_one_zeros(data_te[subtask])):\n",
    "                print(\n",
    "                    '\\ninvalid random seed {} due to one class presented in the {} splitted sets...'.format(seed,\n",
    "                                                                                                            subtask))\n",
    "                print('Changing to another random seed...\\n')\n",
    "                seed = np.random.randint(50, 999999)\n",
    "            else:\n",
    "                print('random seed used in repetition {} is {}'.format(split, seed))\n",
    "                break\n",
    "    else:\n",
    "        training_data, data_te = train_test_split(sub_dataset, test_size=0.1, random_state=seed)\n",
    "        # the training set was further splited into the training set and validation set\n",
    "        data_tr, data_va = train_test_split(training_data, test_size=0.1, random_state=seed)\n",
    "\n",
    "    # prepare data for training\n",
    "    # training set\n",
    "    data_tr_y = data_tr[subtask].values.reshape(-1, 1)\n",
    "    data_tr_x = np.array(data_tr.iloc[:, 1:].values)\n",
    "\n",
    "    # validation set\n",
    "    data_va_y = data_va[subtask].values.reshape(-1, 1)\n",
    "    data_va_x = np.array(data_va.iloc[:, 1:].values)\n",
    "\n",
    "    # test set\n",
    "    data_te_y = data_te[subtask].values.reshape(-1, 1)\n",
    "    data_te_x = np.array(data_te.iloc[:, 1:].values)\n",
    "          \n",
    "    num_fea = data_tr_x.shape[1]\n",
    "    pos_weight = (len(sub_dataset) - sum(sub_dataset[subtask])) / sum(sub_dataset[subtask])\n",
    "    model = LGBMClassifier(\n",
    "                          num_leaves=best_hyper[best_hyper.subtask == subtask].iloc[0,]['num_leaves'],\n",
    "                          learning_rate=best_hyper[best_hyper.subtask == subtask].iloc[0,]['learning_rate'],\n",
    "                          n_estimators=best_hyper[best_hyper.subtask == subtask].iloc[0,]['n_estimators'],\n",
    "                          min_split_gain=best_hyper[best_hyper.subtask == subtask].iloc[0,]['min_split_gain'],\n",
    "                          reg_alpha=best_hyper[best_hyper.subtask == subtask].iloc[0,]['reg_alpha'],\n",
    "                          reg_lambda=best_hyper[best_hyper.subtask == subtask].iloc[0,]['reg_lambda'],\n",
    "                          feature_fraction=best_hyper[best_hyper.subtask == subtask].iloc[0,]['feature_fraction'],\n",
    "                          bagging_fraction=best_hyper[best_hyper.subtask == subtask].iloc[0,]['bagging_fraction'],\n",
    "                          bagging_freq=best_hyper[best_hyper.subtask == subtask].iloc[0,]['bagging_freq'],\n",
    "                          min_child_samples=best_hyper[best_hyper.subtask == subtask].iloc[0,]['min_child_samples'],\n",
    "                          n_jobs=-1, random_state=1) \\\n",
    "        if task_type == 'cla' else LGBMRegressor(\n",
    "                          num_leaves=best_hyper[best_hyper.subtask == subtask].iloc[0,]['num_leaves'],\n",
    "                          learning_rate=best_hyper[best_hyper.subtask == subtask].iloc[0,]['learning_rate'],\n",
    "                          n_estimators=best_hyper[best_hyper.subtask == subtask].iloc[0,]['n_estimators'],\n",
    "                          min_split_gain=best_hyper[best_hyper.subtask == subtask].iloc[0,]['min_split_gain'],\n",
    "                          reg_alpha=best_hyper[best_hyper.subtask == subtask].iloc[0,]['reg_alpha'],\n",
    "                          reg_lambda=best_hyper[best_hyper.subtask == subtask].iloc[0,]['reg_lambda'],\n",
    "                          feature_fraction=best_hyper[best_hyper.subtask == subtask].iloc[0,]['feature_fraction'],\n",
    "                          bagging_fraction=best_hyper[best_hyper.subtask == subtask].iloc[0,]['bagging_fraction'],\n",
    "                          bagging_freq=best_hyper[best_hyper.subtask == subtask].iloc[0,]['bagging_freq'],\n",
    "                          min_child_samples=best_hyper[best_hyper.subtask == subtask].iloc[0,]['min_child_samples'],\n",
    "        n_jobs=6, random_state=1, seed=1)\n",
    "\n",
    "    model.fit(data_tr_x, data_tr_y, eval_metric='auc' if task_type == 'cla' else 'rmse',\n",
    "              eval_set=[(data_va_x, data_va_y)],\n",
    "              early_stopping_rounds=patience, verbose=False)\n",
    "    num_of_compounds = sub_dataset.shape[0]\n",
    "    import pickle\n",
    "    pickle.dump(model, open(\"./model/LGB_\"+str(split)+\".pkl\", \"wb\"))\n",
    "    if task_type == 'cla':\n",
    "        # training set\n",
    "        tr_pred = model.predict_proba(data_tr_x)\n",
    "        tr_results = [split, dataset_label, subtask, 'tr', num_fea, num_of_compounds,\n",
    "                      data_tr_y[data_tr_y == 1].shape[0],\n",
    "                      data_tr_y[data_tr_y == 0].shape[0],\n",
    "                      data_tr_y[data_tr_y == 0].shape[0] / data_tr_y[data_tr_y == 1].shape[0]]\n",
    "        tr_results.extend(statistical(data_tr_y, np.argmax(tr_pred, axis=1), tr_pred[:, 1]))\n",
    "\n",
    "        # validation set\n",
    "        va_pred = model.predict_proba(data_va_x)\n",
    "        va_results = [split, dataset_label, subtask, 'va', num_fea, num_of_compounds,\n",
    "                      data_va_y[data_va_y == 1].shape[0],\n",
    "                      data_va_y[data_va_y == 0].shape[0],\n",
    "                      data_va_y[data_va_y == 0].shape[0] / data_va_y[data_va_y == 1].shape[0]]\n",
    "        va_results.extend(statistical(data_va_y, np.argmax(va_pred, axis=1), va_pred[:, 1]))\n",
    "\n",
    "        # test set\n",
    "        te_pred = model.predict_proba(data_te_x)\n",
    "        te_results = [split, dataset_label, subtask, 'te', num_fea, num_of_compounds,\n",
    "                      data_te_y[data_te_y == 1].shape[0],\n",
    "                      data_te_y[data_te_y == 0].shape[0],\n",
    "                      data_te_y[data_te_y == 0].shape[0] / data_te_y[data_te_y == 1].shape[0]]\n",
    "        te_results.extend(statistical(data_te_y, np.argmax(te_pred, axis=1), te_pred[:, 1]))\n",
    "    else:\n",
    "        # training set\n",
    "        tr_pred = model.predict(data_tr_x)\n",
    "        tr_results = [split, dataset_label, subtask, 'tr', num_fea, num_of_compounds,\n",
    "                      np.sqrt(mean_squared_error(data_tr_y, tr_pred)), r2_score(data_tr_y, tr_pred),\n",
    "                      mean_absolute_error(data_tr_y, tr_pred)]\n",
    "\n",
    "        # validation set\n",
    "        va_pred = model.predict(data_va_x)\n",
    "        va_results = [split, dataset_label, subtask, 'va', num_fea, num_of_compounds,\n",
    "                      np.sqrt(mean_squared_error(data_va_y, va_pred)), r2_score(data_va_y, va_pred),\n",
    "                      mean_absolute_error(data_va_y, va_pred)]\n",
    "\n",
    "        # test set\n",
    "        te_pred = model.predict(data_te_x)\n",
    "        te_results = [split, dataset_label, subtask, 'te', num_fea, num_of_compounds,\n",
    "                      np.sqrt(mean_squared_error(data_te_y, te_pred)), r2_score(data_te_y, te_pred),\n",
    "                      mean_absolute_error(data_te_y, te_pred)]\n",
    "    return tr_results, va_results, te_results\n",
    "\n",
    "\n",
    "for subtask in tasks:\n",
    "    cols = [subtask]\n",
    "    cols.extend(dataset.columns[(len(tasks) + 1):])\n",
    "    # cols.extend(dataset.columns[(617+1):])\n",
    "    sub_dataset = dataset[cols]\n",
    "\n",
    "    # detect the NA in the subtask (y cloumn)\n",
    "    rm_index = sub_dataset[subtask][sub_dataset[subtask].isnull()].index\n",
    "    sub_dataset.drop(index=rm_index, inplace=True)\n",
    "\n",
    "    # remove the features with na\n",
    "    sub_dataset = sub_dataset.dropna(axis=1)\n",
    "\n",
    "    # *******************\n",
    "    # demension reduction\n",
    "    # *******************\n",
    "    # Removing features with low variance\n",
    "    # threshold = 0.05\n",
    "    data_fea_var = sub_dataset.iloc[:, 1:].var()\n",
    "    del_fea1 = list(data_fea_var[data_fea_var <= 0.05].index)\n",
    "    sub_dataset.drop(columns=del_fea1, inplace=True)\n",
    "\n",
    "    # pair correlations\n",
    "    # threshold = 0.95\n",
    "    data_fea_corr = sub_dataset.iloc[:, 1:].corr()\n",
    "    del_fea2_col = []\n",
    "    del_fea2_ind = []\n",
    "    length = data_fea_corr.shape[1]\n",
    "    for i in range(length):\n",
    "        for j in range(i + 1, length):\n",
    "            if abs(data_fea_corr.iloc[i, j]) >= 0.95:\n",
    "                del_fea2_col.append(data_fea_corr.columns[i])\n",
    "                del_fea2_ind.append(data_fea_corr.index[j])\n",
    "    sub_dataset.drop(columns=del_fea2_ind, inplace=True)\n",
    "\n",
    "    # standardize the features\n",
    "    cols_ = list(sub_dataset.columns)[1:]\n",
    "    if not ecfp :\n",
    "        sub_dataset[cols_] = sub_dataset[cols_].apply(standardize, axis=0)\n",
    "\n",
    "    # for split in range(1, splits+1):\n",
    "    pool = multiprocessing.Pool(num_pools)\n",
    "    res = pool.starmap(best_model_runing, zip(range(1, repetitions + 1)))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    for item in res:\n",
    "        for i in range(3):\n",
    "            pd_res.append(item[i])\n",
    "if task_type == 'cla':\n",
    "    stat_res = pd.DataFrame(pd_res, columns=['split', 'dataset', 'subtask', 'set',\n",
    "                                             'num_of_retained_feature',\n",
    "                                             'num_of_compounds', 'postives',\n",
    "                                             'negtives', 'negtives/postives',\n",
    "                                             'tn', 'fp', 'fn', 'tp', 'se', 'sp',\n",
    "                                             'auc_prc', 'acc', 'auc_roc','recall','precision','f1','kappa','mcc'])\n",
    "else:\n",
    "    stat_res = pd.DataFrame(pd_res, columns=['split', 'dataset', 'subtask', 'set',\n",
    "                                             'num_of_retained_feature',\n",
    "                                             'num_of_compounds', 'rmse', 'r2', 'mae'])\n",
    "stat_res.to_csv('./model/' + dataset_label + '_LGB_statistical_results_split50.csv', index=0)\n",
    "# single tasks\n",
    "if len(tasks) == 1:\n",
    "    args = {'data_label': dataset_label, 'metric': 'auc_roc' if task_type == 'cla' else 'rmse', 'model': 'LGB'}\n",
    "    print('{}_{}: the mean {} for the training set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                     args['metric'], np.mean(\n",
    "            stat_res[stat_res['set'] == 'tr'][args['metric']]), np.std(\n",
    "            stat_res[stat_res['set'] == 'tr'][args['metric']])))\n",
    "    print(\n",
    "        '{}_{}: the mean {} for the validation set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                     args['metric'], np.mean(\n",
    "                stat_res[stat_res['set'] == 'va'][args['metric']]), np.std(\n",
    "                stat_res[stat_res['set'] == 'va'][args['metric']])))\n",
    "    print('{}_{}: the mean {} for the test set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                 args['metric'], np.mean(\n",
    "            stat_res[stat_res['set'] == 'te'][args['metric']]), np.std(\n",
    "            stat_res[stat_res['set'] == 'te'][args['metric']])))\n",
    "# multi-tasks\n",
    "else:\n",
    "    args = {'data_label': dataset_label, 'metric': 'auc_roc' if dataset_label != 'muv' else 'auc_prc', 'model': 'LGB'}\n",
    "    tr_acc = np.zeros(repetitions)\n",
    "    va_acc = np.zeros(repetitions)\n",
    "    te_acc = np.zeros(repetitions)\n",
    "    for subtask in tasks:\n",
    "        tr = stat_res[stat_res['set'] == 'tr']\n",
    "        tr_acc = tr_acc + tr[tr['subtask'] == subtask][args['metric']].values\n",
    "\n",
    "        va = stat_res[stat_res['set'] == 'va']\n",
    "        va_acc = va_acc + va[va['subtask'] == subtask][args['metric']].values\n",
    "\n",
    "        te = stat_res[stat_res['set'] == 'te']\n",
    "        te_acc = te_acc + te[te['subtask'] == subtask][args['metric']].values\n",
    "    tr_acc = tr_acc / len(tasks)\n",
    "    va_acc = va_acc / len(tasks)\n",
    "    te_acc = te_acc / len(tasks)\n",
    "    print('{}_{}: the mean {} for the training set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                     args['metric'], np.mean(tr_acc),\n",
    "                                                                                     np.std(tr_acc)))\n",
    "    print(\n",
    "        '{}_{}: the mean {} for the validation set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                     args['metric'], np.mean(va_acc),\n",
    "                                                                                     np.std(va_acc)))\n",
    "    print('{}_{}: the mean {} for the test set is {:.3f} with std {:.3f}'.format(args['data_label'], args['model'],\n",
    "                                                                                 args['metric'], np.mean(te_acc),\n",
    "                                                                                 np.std(te_acc)))\n",
    "end = time.time()  # get the end time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50bc4eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the elapsed time is: 2.9601318382554584 H\n"
     ]
    }
   ],
   "source": [
    "# acc auc_roc recall precision f1 kappa mcc\n",
    "acc_str = 'acc of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['acc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['acc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['acc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['acc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['acc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['acc']),\n",
    ")\n",
    "auc_str = 'auc_roc of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['auc_roc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['auc_roc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['auc_roc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['auc_roc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['auc_roc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['auc_roc']),\n",
    ")\n",
    "recall_str = 'recall of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['recall']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['recall']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['recall']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['recall']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['recall']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['recall']),\n",
    ")\n",
    "precision_str = 'precision of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['precision']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['precision']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['precision']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['precision']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['precision']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['precision']),\n",
    ")\n",
    "f1_str = 'f1 of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['f1']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['f1']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['f1']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['f1']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['f1']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['f1']),\n",
    ")\n",
    "kappa_str = 'kappa of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['kappa']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['kappa']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['kappa']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['kappa']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['kappa']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['kappa']),\n",
    ")\n",
    "mcc_str = 'mcc of training set is {:.3f}±{:.3f}, validation set is {:.3f}±{:.3f}, test set is {:.3f}±{:.3f}'.format(\n",
    "                np.mean(stat_res[stat_res['set'] == 'tr']['mcc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'tr']['mcc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'va']['mcc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'va']['mcc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['mcc']), \n",
    "                np.std(stat_res[stat_res['set'] == 'te']['mcc']),\n",
    ")\n",
    "print('the elapsed time is:', (end - start)/3600, 'H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfb4f708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc of training set is 0.973±0.020, validation set is 0.910±0.011, test set is 0.909±0.007\n",
      "auc_roc of training set is 0.993±0.008, validation set is 0.939±0.011, test set is 0.935±0.005\n",
      "recall of training set is 0.871±0.094, validation set is 0.638±0.034, test set is 0.652±0.044\n",
      "precision of training set is 0.982±0.016, validation set is 0.863±0.042, test set is 0.848±0.035\n",
      "f1 of training set is 0.921±0.060, validation set is 0.732±0.020, test set is 0.736±0.022\n",
      "kappa of training set is 0.905±0.072, validation set is 0.680±0.025, test set is 0.682±0.025\n",
      "mcc of training set is 0.909±0.068, validation set is 0.692±0.023, test set is 0.692±0.020\n"
     ]
    }
   ],
   "source": [
    "print(acc_str)\n",
    "print(auc_str)\n",
    "print(recall_str)\n",
    "print(precision_str)\n",
    "print(f1_str)\n",
    "print(kappa_str)\n",
    "print(mcc_str)\n",
    "with open('output/output_lgb.txt', 'w') as f:\n",
    "    f.write(acc_str+'\\n')\n",
    "    f.write(auc_str+'\\n')\n",
    "    f.write(recall_str+'\\n')\n",
    "    f.write(precision_str+'\\n')\n",
    "    f.write(f1_str+'\\n')\n",
    "    f.write(kappa_str+'\\n')\n",
    "    f.write(mcc_str+'\\n')\n",
    "    f.write(str(cols_)+'\\n')\n",
    "cols_ = pd.DataFrame(cols_)\n",
    "cols_.to_csv('output/output_lgb_cols.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee4ebafb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model: LGB</th>\n",
       "      <th>Train</th>\n",
       "      <th>Tr_STD</th>\n",
       "      <th>Validation</th>\n",
       "      <th>Va_STD</th>\n",
       "      <th>Test</th>\n",
       "      <th>Te_STD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acc</td>\n",
       "      <td>0.972893</td>\n",
       "      <td>0.019818</td>\n",
       "      <td>0.909636</td>\n",
       "      <td>0.010819</td>\n",
       "      <td>0.909165</td>\n",
       "      <td>0.007365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>auc_roc</td>\n",
       "      <td>0.992927</td>\n",
       "      <td>0.008108</td>\n",
       "      <td>0.938895</td>\n",
       "      <td>0.011165</td>\n",
       "      <td>0.934839</td>\n",
       "      <td>0.004849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.870943</td>\n",
       "      <td>0.094252</td>\n",
       "      <td>0.637977</td>\n",
       "      <td>0.033637</td>\n",
       "      <td>0.652420</td>\n",
       "      <td>0.043889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.981980</td>\n",
       "      <td>0.015803</td>\n",
       "      <td>0.863215</td>\n",
       "      <td>0.042261</td>\n",
       "      <td>0.848308</td>\n",
       "      <td>0.034877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.921126</td>\n",
       "      <td>0.060476</td>\n",
       "      <td>0.732296</td>\n",
       "      <td>0.020015</td>\n",
       "      <td>0.735740</td>\n",
       "      <td>0.022374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kappa</td>\n",
       "      <td>0.904964</td>\n",
       "      <td>0.072060</td>\n",
       "      <td>0.679507</td>\n",
       "      <td>0.025044</td>\n",
       "      <td>0.682193</td>\n",
       "      <td>0.024617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mcc</td>\n",
       "      <td>0.908685</td>\n",
       "      <td>0.068212</td>\n",
       "      <td>0.691724</td>\n",
       "      <td>0.023395</td>\n",
       "      <td>0.691945</td>\n",
       "      <td>0.019983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>auc_prc</td>\n",
       "      <td>0.978611</td>\n",
       "      <td>0.023697</td>\n",
       "      <td>0.851036</td>\n",
       "      <td>0.020240</td>\n",
       "      <td>0.845290</td>\n",
       "      <td>0.013031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model: LGB      Train    Tr_STD  Validation    Va_STD      Test    Te_STD\n",
       "0         acc  0.972893  0.019818    0.909636  0.010819  0.909165  0.007365\n",
       "1     auc_roc  0.992927  0.008108    0.938895  0.011165  0.934839  0.004849\n",
       "2      recall  0.870943  0.094252    0.637977  0.033637  0.652420  0.043889\n",
       "3   precision  0.981980  0.015803    0.863215  0.042261  0.848308  0.034877\n",
       "4          f1  0.921126  0.060476    0.732296  0.020015  0.735740  0.022374\n",
       "5       kappa  0.904964  0.072060    0.679507  0.025044  0.682193  0.024617\n",
       "6         mcc  0.908685  0.068212    0.691724  0.023395  0.691945  0.019983\n",
       "7     auc_prc  0.978611  0.023697    0.851036  0.020240  0.845290  0.013031"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "dict1 = {\"Model: LGB \":['acc','auc_roc','recall','precision','f1','kappa','mcc','auc_prc'],\n",
    "         \"Train\":[np.mean(stat_res[stat_res['set'] == 'tr']['acc']),np.mean(stat_res[stat_res['set'] == 'tr']['auc_roc']),\n",
    "                  np.mean(stat_res[stat_res['set'] == 'tr']['recall']),np.mean(stat_res[stat_res['set'] == 'tr']['precision']),\n",
    "                  np.mean(stat_res[stat_res['set'] == 'tr']['f1']),np.mean(stat_res[stat_res['set'] == 'tr']['kappa']),\n",
    "                  np.mean(stat_res[stat_res['set'] == 'tr']['mcc']),np.mean(stat_res[stat_res['set'] == 'tr']['auc_prc']),                                     \n",
    "                 ],\n",
    "         \"Tr_STD\":[np.std(stat_res[stat_res['set'] == 'tr']['acc']),np.std(stat_res[stat_res['set'] == 'tr']['auc_roc']),\n",
    "                  np.std(stat_res[stat_res['set'] == 'tr']['recall']),np.std(stat_res[stat_res['set'] == 'tr']['precision']),\n",
    "                  np.std(stat_res[stat_res['set'] == 'tr']['f1']),np.std(stat_res[stat_res['set'] == 'tr']['kappa']),\n",
    "                  np.std(stat_res[stat_res['set'] == 'tr']['mcc']),np.std(stat_res[stat_res['set'] == 'tr']['auc_prc']),],\n",
    "         \"Validation\":[np.mean(stat_res[stat_res['set'] == 'va']['acc']),np.mean(stat_res[stat_res['set'] == 'va']['auc_roc']),\n",
    "                      np.mean(stat_res[stat_res['set'] == 'va']['recall']),np.mean(stat_res[stat_res['set'] == 'va']['precision']),\n",
    "                      np.mean(stat_res[stat_res['set'] == 'va']['f1']),np.mean(stat_res[stat_res['set'] == 'va']['kappa']),\n",
    "                      np.mean(stat_res[stat_res['set'] == 'va']['mcc']),np.mean(stat_res[stat_res['set'] == 'va']['auc_prc'])],\n",
    "         \"Va_STD\":[np.std(stat_res[stat_res['set'] == 'va']['acc']),np.std(stat_res[stat_res['set'] == 'va']['auc_roc']),\n",
    "                  np.std(stat_res[stat_res['set'] == 'va']['recall']),np.std(stat_res[stat_res['set'] == 'va']['precision']),\n",
    "                  np.std(stat_res[stat_res['set'] == 'va']['f1']),np.std(stat_res[stat_res['set'] == 'va']['kappa']),\n",
    "                  np.std(stat_res[stat_res['set'] == 'va']['mcc']),np.std(stat_res[stat_res['set'] == 'va']['auc_prc'])],\n",
    "         \"Test\":[np.mean(stat_res[stat_res['set'] == 'te']['acc']),np.mean(stat_res[stat_res['set'] == 'te']['auc_roc']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['recall']),np.mean(stat_res[stat_res['set'] == 'te']['precision']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['f1']),np.mean(stat_res[stat_res['set'] == 'te']['kappa']),\n",
    "                np.mean(stat_res[stat_res['set'] == 'te']['mcc']),np.mean(stat_res[stat_res['set'] == 'te']['auc_prc'])],\n",
    "          \"Te_STD\":[np.std(stat_res[stat_res['set'] == 'te']['acc']),np.std(stat_res[stat_res['set'] == 'te']['auc_roc']),\n",
    "                   np.std(stat_res[stat_res['set'] == 'te']['recall']),np.std(stat_res[stat_res['set'] == 'te']['precision']),\n",
    "                   np.std(stat_res[stat_res['set'] == 'te']['f1']),np.std(stat_res[stat_res['set'] == 'te']['kappa']),\n",
    "                   np.std(stat_res[stat_res['set'] == 'te']['mcc']),np.std(stat_res[stat_res['set'] == 'te']['auc_prc']),]}\n",
    "dict1 = collections.OrderedDict(dict1)\n",
    "df = pd.DataFrame(dict1,index = None)\n",
    "df.to_csv('output/output_lgb.csv',index = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5f062d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>num_leaves</th>\n",
       "      <td>248.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.143406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_estimators</th>\n",
       "      <td>189.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_split_gain</th>\n",
       "      <td>0.091766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_alpha</th>\n",
       "      <td>2.788000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reg_lambda</th>\n",
       "      <td>2.899420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_fraction</th>\n",
       "      <td>0.782207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagging_fraction</th>\n",
       "      <td>0.535848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bagging_freq</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_child_samples</th>\n",
       "      <td>78.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Values\n",
       "num_leaves         248.000000\n",
       "learning_rate        0.143406\n",
       "n_estimators       189.000000\n",
       "min_split_gain       0.091766\n",
       "reg_alpha            2.788000\n",
       "reg_lambda           2.899420\n",
       "feature_fraction     0.782207\n",
       "bagging_fraction     0.535848\n",
       "bagging_freq         2.000000\n",
       "min_child_samples   78.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option ( 'display.width', None)\n",
    "pd.set_option ( 'display.max_columns', None) #显示全部列\n",
    "hyper_parameters = best_hyper.iloc[0:1,8:-14].T\n",
    "hyper_parameters.rename(columns={0:'Values'},inplace=True) \n",
    "hyper_parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-dgl]",
   "language": "python",
   "name": "conda-env-anaconda3-dgl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
