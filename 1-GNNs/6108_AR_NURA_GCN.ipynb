{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from copy import deepcopy\n",
    "from dgllife.utils import Meter, EarlyStopping\n",
    "from hyperopt import fmin, tpe\n",
    "from shutil import copyfile\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hyper import init_hyper_space\n",
    "from utils import get_configure, mkdir_p, init_trial_path, \\\n",
    "    split_dataset, collate_molgraphs, load_model, predict, init_featurizer, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "hyperopt.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "patienceNum = 50\n",
    "# batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import roc_curve,roc_auc_score, confusion_matrix, precision_recall_curve, auc, mean_squared_error, \\\n",
    "    r2_score, mean_absolute_error,cohen_kappa_score,accuracy_score,f1_score,matthews_corrcoef,precision_score,recall_score\n",
    "\n",
    "#Calculate the metrics.\n",
    "class AllMeter(object):\n",
    "    \n",
    "    def statistical(self):\n",
    "        \n",
    "        y_test = pd.DataFrame(self.y_test)\n",
    "        y_predict = pd.DataFrame(self.y_predict)\n",
    "        fpr, tpr, threshold = roc_curve(y_test, y_predict)\n",
    "        auc_prc = auc(precision_recall_curve(y_test, y_predict, pos_label=1)[1],\n",
    "                              precision_recall_curve(y_test, y_predict, pos_label=1)[0])\n",
    "        auc_roc = auc(fpr, tpr)\n",
    "        output_tran = []\n",
    "        for x in y_predict[0]:\n",
    "            if x > 0.5:\n",
    "                output_tran.append(1)\n",
    "            else:\n",
    "                output_tran.append(0)\n",
    "        acc = accuracy_score(y_test, output_tran)\n",
    "        recall = recall_score(y_test, output_tran)\n",
    "        precision = precision_score(y_test, output_tran)\n",
    "        f1 = f1_score(y_test, output_tran)\n",
    "        kappa = cohen_kappa_score(y_test,output_tran)   \n",
    "        mcc = matthews_corrcoef(y_test,output_tran)\n",
    "        \n",
    "        c_mat = confusion_matrix(y_test, output_tran)            \n",
    "        tn, fp, fn, tp = list(c_mat.flatten())\n",
    "        se = tp / (tp + fn)\n",
    "        sp = tn / (tn + fp)\n",
    "        acc_ = (tp + tn) / (tn + fp + fn + tp)          \n",
    "        recall_ = se\n",
    "        precision_ = tp / (tp + fp)\n",
    "        f1_ = 2 * (precision * recall) / (precision + recall) # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "        mcc_ = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn) + 1e-8)\n",
    "        \n",
    "        scores_dict = {}\n",
    "        scores_dict['auc_prc'] = auc_prc\n",
    "        scores_dict['acc'] = acc\n",
    "        scores_dict['auc_roc'] = auc_roc\n",
    "        scores_dict['recall'] = recall\n",
    "        scores_dict['precision'] = precision\n",
    "        scores_dict['f1'] = f1\n",
    "        scores_dict['kappa'] = kappa\n",
    "        scores_dict['mcc'] = mcc \n",
    "\n",
    "        print(scores_dict) \n",
    "        print({'Confusion matrixï¼š acc':acc_,'recall_':recall_,'precision_':precision_,'f1_':f1_,'mcc_':mcc_})\n",
    "        return scores_dict\n",
    "         \n",
    "    \n",
    "    def __init__(self, mean=None, std=None):\n",
    "        self.y_predict = []\n",
    "        self.y_test = []\n",
    "            \n",
    "    def update(self, output, label, mask=None):\n",
    "            output = torch.sigmoid(output)\n",
    "            output = output.cpu().detach().numpy()\n",
    "            label = label.cpu().detach().numpy()\n",
    "            for i in output:\n",
    "                self.y_predict.append(i)\n",
    "            for j in label:\n",
    "                self.y_test.append(j)        \n",
    "    def compute_metric(self, metric_name, reduction='mean'):\n",
    "        if metric_name == 'getAllMetrics':\n",
    "            return self.statistical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_a_train_epoch(args, epoch, model, data_loader, loss_criterion, optimizer):\n",
    "    model.train()\n",
    "    train_meter = Meter()\n",
    "    all_train_meter = AllMeter()\n",
    "    for batch_id, batch_data in enumerate(data_loader):\n",
    "        smiles, bg, labels, masks = batch_data\n",
    "        if len(smiles) == 1:\n",
    "            # Avoid potential issues with batch normalization\n",
    "            continue\n",
    "\n",
    "        labels, masks = labels.to(args['device']), masks.to(args['device'])\n",
    "        logits = predict(args, model, bg)\n",
    "        # Mask non-existing labels\n",
    "        loss = (loss_criterion(logits, labels) * (masks != 0).float()).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_meter.update(logits, labels, masks)\n",
    "        all_train_meter.update(logits, labels)\n",
    "        if batch_id % args['print_every'] == 0:\n",
    "            print('epoch {:d}/{:d}, batch {:d}/{:d}, loss {:.4f}'.format(\n",
    "                epoch + 1, args['num_epochs'], batch_id + 1, len(data_loader), loss.item()))\n",
    "    train_score = np.mean(train_meter.compute_metric(args['metric']))\n",
    "    print('epoch {:d}/{:d}, training {} {:.4f}'.format(\n",
    "        epoch + 1, args['num_epochs'], args['metric'], train_score))\n",
    "    roc_score = np.mean(train_meter.compute_metric(args['metric']))  \n",
    "    prc_score = np.mean(train_meter.compute_metric('pr_auc_score'))  \n",
    "    all_score = all_train_meter.compute_metric('getAllMetrics') \n",
    "#     return {'roc_auc_score': roc_score, 'pr_auc_score': prc_score, 'all_score': all_score}\n",
    "\n",
    "def run_an_eval_epoch(args, model, data_loader):\n",
    "    model.eval()\n",
    "    eval_meter = Meter()\n",
    "    all_eval_meter = AllMeter()\n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch_data in enumerate(data_loader):\n",
    "            smiles, bg, labels, masks = batch_data\n",
    "            labels = labels.to(args['device'])\n",
    "            logits = predict(args, model, bg)\n",
    "            eval_meter.update(logits, labels, masks)\n",
    "            all_eval_meter.update(logits, labels, masks)\n",
    "    roc_score = np.mean(eval_meter.compute_metric(args['metric']))  # in case of multi-tasks\n",
    "    prc_score = np.mean(eval_meter.compute_metric('pr_auc_score'))  # in case of multi-task\n",
    "    all_score = all_eval_meter.compute_metric('getAllMetrics') \n",
    "    return {'roc_auc_score': roc_score, 'pr_auc_score': prc_score, 'all_score': all_score}\n",
    "#     return np.mean(eval_meter.compute_metric(args['metric']))\n",
    "\n",
    "def main(args, exp_config, train_set, val_set, test_set):\n",
    "    # Record settings\n",
    "    exp_config.update({\n",
    "        'model': args['model'],\n",
    "        'n_tasks': args['n_tasks'],\n",
    "        'atom_featurizer_type': args['atom_featurizer_type'],\n",
    "        'bond_featurizer_type': args['bond_featurizer_type'],\n",
    "#         'patience': patienceNum,\n",
    "#         'batch_size':batch_size\n",
    "    })\n",
    "    if args['atom_featurizer_type'] != 'pre_train':\n",
    "        exp_config['in_node_feats'] = args['node_featurizer'].feat_size()\n",
    "    if args['edge_featurizer'] is not None and args['bond_featurizer_type'] != 'pre_train':\n",
    "        exp_config['in_edge_feats'] = args['edge_featurizer'].feat_size()\n",
    "\n",
    "    # Set up directory for saving results\n",
    "    args = init_trial_path(args)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=exp_config['batch_size'], shuffle=True,\n",
    "                              collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=exp_config['batch_size'],\n",
    "                            collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=exp_config['batch_size'],\n",
    "                             collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    model = load_model(exp_config).to(args['device'])\n",
    "\n",
    "    loss_criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    optimizer = Adam(model.parameters(), lr=exp_config['lr'],\n",
    "                     weight_decay=exp_config['weight_decay'])\n",
    "    stopper = EarlyStopping(patience=exp_config['patience'],\n",
    "                            filename=args['trial_path'] + '/model.pth',\n",
    "                            metric=args['metric'])\n",
    "\n",
    "    for epoch in range(args['num_epochs']):\n",
    "        # Train\n",
    "        run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer)\n",
    "\n",
    "        # Validation and early stop\n",
    "        val_score = run_an_eval_epoch(args, model, val_loader)\n",
    "        early_stop = stopper.step(val_score['roc_auc_score'], model)\n",
    "        print('epoch {:d}/{:d}, validation {} {:.4f}, best validation {} {:.4f}'.format(\n",
    "            epoch + 1, args['num_epochs'], args['metric'],\n",
    "            val_score['roc_auc_score'], args['metric'], stopper.best_score))\n",
    "\n",
    "        if early_stop:\n",
    "            break\n",
    "\n",
    "    stopper.load_checkpoint(model)\n",
    "    test_score = run_an_eval_epoch(args, model, test_loader)\n",
    "    print('test {} {:.4f}'.format(args['metric'], test_score['roc_auc_score']))\n",
    "\n",
    "    with open(args['trial_path'] + '/eval.txt', 'w') as f:\n",
    "        f.write('Best val {}: {}\\n'.format(args['metric'], stopper.best_score))\n",
    "        f.write('Test {}: {}\\n'.format(args['metric'], test_score['roc_auc_score']))\n",
    "\n",
    "    with open(args['trial_path'] + '/configure.json', 'w') as f:\n",
    "        json.dump(exp_config, f, indent=2)\n",
    "\n",
    "    return args['trial_path'], stopper.best_score\n",
    "\n",
    "def bayesian_optimization(args, train_set, val_set, test_set):\n",
    "    # Run grid search\n",
    "    results = []\n",
    "\n",
    "    candidate_hypers = init_hyper_space(args['model'])\n",
    "\n",
    "    def objective(hyperparams):\n",
    "        configure = deepcopy(args)\n",
    "        trial_path, val_metric = main(configure, hyperparams, train_set, val_set, test_set)\n",
    "\n",
    "        if args['metric'] in ['roc_auc_score', 'pr_auc_score']:\n",
    "            # Maximize ROCAUC is equivalent to minimize the negative of it\n",
    "            val_metric_to_minimize = -1 * val_metric\n",
    "        else:\n",
    "            val_metric_to_minimize = val_metric\n",
    "\n",
    "        results.append((trial_path, val_metric_to_minimize))\n",
    "\n",
    "        return val_metric_to_minimize\n",
    "\n",
    "    fmin(objective, candidate_hypers, algo=tpe.suggest, max_evals=args['num_evals'])\n",
    "    results.sort(key=lambda tup: tup[1])\n",
    "    best_trial_path, best_val_metric = results[0]\n",
    "\n",
    "    return best_trial_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-c', '--csv-path', type=str, required=True,\n",
    "                    help='Path to a csv file for loading a dataset')\n",
    "parser.add_argument('-sc', '--smiles-column', type=str, required=True,\n",
    "                    help='Header for the SMILES column in the CSV file')\n",
    "parser.add_argument('-lv', '--log-values', action='store_true', default=False,\n",
    "                    help='Whether to take logarithm of the labels for modeling')\n",
    "parser.add_argument('-t', '--task-names', default=None, type=str,\n",
    "                    help='Header for the tasks to model. If None, we will model '\n",
    "                         'all the columns except for the smiles_column in the CSV file. '\n",
    "                         '(default: None)')\n",
    "parser.add_argument('-s', '--split',\n",
    "                    choices=['scaffold_decompose', 'scaffold_smiles', 'random'],\n",
    "                    default='scaffold_smiles',\n",
    "                    help='Dataset splitting method (default: scaffold_smiles). For scaffold '\n",
    "                         'split based on rdkit.Chem.AllChem.MurckoDecompose, '\n",
    "                         'use scaffold_decompose. For scaffold split based on '\n",
    "                         'rdkit.Chem.Scaffolds.MurckoScaffold.MurckoScaffoldSmiles, '\n",
    "                         'use scaffold_smiles.')\n",
    "parser.add_argument('-sr', '--split-ratio', default='0.8,0.1,0.1', type=str,\n",
    "                    help='Proportion of the dataset to use for training, validation and test '\n",
    "                         '(default: 0.8,0.1,0.1)')\n",
    "parser.add_argument('-me', '--metric', choices=['roc_auc_score', 'pr_auc_score'],\n",
    "                        default='roc_auc_score',\n",
    "                        help='Metric for evaluation (default: roc_auc_score)')\n",
    "parser.add_argument('-mo', '--model', choices=['GCN', 'GAT', 'Weave', 'MPNN', 'AttentiveFP',\n",
    "                                               'gin_supervised_contextpred',\n",
    "                                               'gin_supervised_infomax',\n",
    "                                               'gin_supervised_edgepred',\n",
    "                                               'gin_supervised_masking',\n",
    "                                               'NF'],\n",
    "                    default='GCN', help='Model to use (default: GCN)')\n",
    "parser.add_argument('-a', '--atom-featurizer-type', choices=['canonical', 'attentivefp'],\n",
    "                    default='canonical',\n",
    "                    help='Featurization for atoms (default: canonical)')\n",
    "parser.add_argument('-b', '--bond-featurizer-type', choices=['canonical', 'attentivefp'],\n",
    "                    default='canonical',\n",
    "                    help='Featurization for bonds (default: canonical)')\n",
    "parser.add_argument('-n', '--num-epochs', type=int, default=1000,\n",
    "                    help='Maximum number of epochs allowed for training. '\n",
    "                         'We set a large number by default as early stopping '\n",
    "                         'will be performed. (default: 1000)')\n",
    "parser.add_argument('-nw', '--num-workers', type=int, default=0,\n",
    "                    help='Number of processes for data loading (default: 0)')\n",
    "parser.add_argument('-pe', '--print-every', type=int, default=20,\n",
    "                    help='Print the training progress every X mini-batches')\n",
    "parser.add_argument('-p', '--result-path', type=str, default='regression_results',\n",
    "                    help='Path to save training results (default: regression_results)')\n",
    "parser.add_argument('-ne', '--num-evals', type=int, default=None,\n",
    "                    help='Number of trials for hyperparameter search (default: None)')\n",
    "parser.add_argument('-au', '--augmentation', action='store_true', default=False,\n",
    "                    help='Whether to augmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPUNum = '0'\n",
    "repetitions = 10\n",
    "seed = 0 \n",
    "args = parser.parse_args(args=['--csv-path','data/5_AR_6108_NURA.csv',\n",
    "                               '--task-names','label',\n",
    "                               '--smiles-column','smiles',\n",
    "                               '--result-path','result/6108_AR_NURA_GCN_0920',\n",
    "                               '--num-evals','50',\n",
    "                               '--num-epochs','300',\n",
    "#                                '--split-ratio',\n",
    "                                '--split','random',                     \n",
    "                               '--metric','roc_auc_score',\n",
    "                               '--model','GCN',\n",
    "#                                '--atom-featurizer-type','attentivefp',\n",
    "#                                '--bond-featurizer-type','attentivefp',\n",
    "#                                  '--augmentation',\n",
    "#                                '--num-workers',\n",
    "#                                '--print-every',\n",
    "                                  ]).__dict__\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def del_file(filepath):\n",
    "    del_list = os.listdir(filepath)\n",
    "    for f in del_list:\n",
    "        file_path = os.path.join(filepath, f)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "            \n",
    "path_data = args['result_path']\n",
    "\n",
    "if not os.path.exists(path_data):\n",
    "    os.makedirs(path_data)\n",
    "\n",
    "del_file(path_data)\n",
    "\n",
    "dirs = args['result_path']+'/saved_model'\n",
    "\n",
    "if not os.path.exists(dirs):\n",
    "    os.makedirs(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from maxsmi.augmentation_strategies import no_augmentation\n",
    "from maxsmi.augmentation_strategies import augmentation_with_duplication\n",
    "from maxsmi.augmentation_strategies import augmentation_without_duplication\n",
    "from maxsmi.augmentation_strategies import (\n",
    "    augmentation_with_reduced_duplication\n",
    ")\n",
    "from maxsmi.augmentation_strategies import augmentation_maximum_estimation\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def data_augmentation(dataSet,args):\n",
    "    smi_col = args['smiles_column']\n",
    "    task_names = args['task_names'][0]\n",
    "    df_empty = pd.DataFrame(columns=[smi_col,task_names])\n",
    "    for index,row in dataSet.iterrows():\n",
    "        smiles = row[smi_col]\n",
    "        non_duplicated_smiles = augmentation_with_reduced_duplication(smiles, 50)\n",
    "        for data in non_duplicated_smiles:\n",
    "            df = pd.DataFrame({smi_col:data,task_names:float(row[task_names])},index=[0])\n",
    "            df_empty = df_empty.append(df,ignore_index=True)\n",
    "    return df_empty\n",
    "\n",
    "def split_dataset_augmentation(my_df,seed,args):\n",
    "    \n",
    "    training_data, data_test = train_test_split(my_df, test_size=0.1, random_state=seed)\n",
    "    data_train, data_val = train_test_split(training_data, test_size=0.1, random_state=seed)\n",
    "    \n",
    "    data_test = data_augmentation(data_test,args)\n",
    "    data_test = shuffle(data_test,random_state=seed) \n",
    "    data_train = data_augmentation(data_train,args)\n",
    "    data_train = shuffle(data_train,random_state=seed)\n",
    "    data_val = data_augmentation(data_val,args)\n",
    "    data_val = shuffle(data_val,random_state=seed)\n",
    "    test_set = load_dataset(args, data_test)\n",
    "    train_set = load_dataset(args, data_train)\n",
    "    val_set = load_dataset(args, data_val)\n",
    "       \n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    args['device'] = torch.device('cuda:'+ GPUNum)\n",
    "else:\n",
    "    args['device'] = torch.device('cpu')\n",
    "\n",
    "if args['task_names'] is not None:\n",
    "    args['task_names'] = args['task_names'].split(',')\n",
    "\n",
    "args = init_featurizer(args)\n",
    "df = pd.read_csv(args['csv_path'])\n",
    "mkdir_p(args['result_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args['augmentation']:\n",
    "    train_set, val_set, test_set = split_dataset_augmentation(df,seed,args)\n",
    "    args['n_tasks'] = train_set.n_tasks\n",
    "else:\n",
    "    dataset = load_dataset(args, df)\n",
    "    train_set, val_set, test_set = split_dataset(args, dataset,seed)\n",
    "    args['n_tasks'] = dataset.n_tasks\n",
    "    \n",
    "# Whether to take the logarithm of labels for narrowing the range of values\n",
    "if args['log_values']:\n",
    "    train_set.labels = train_set.labels.log()\n",
    "    val_set.labels = val_set.labels.log()\n",
    "    test_set.labels = test_set.labels.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args['num_evals'] is not None:\n",
    "    assert args['num_evals'] > 0, 'Expect the number of hyperparameter search trials to ' \\\n",
    "                                  'be greater than 0, got {:d}'.format(args['num_evals'])\n",
    "    print('Start hyperparameter search with Bayesian '\n",
    "          'optimization for {:d} trials'.format(args['num_evals']))\n",
    "    trial_path = bayesian_optimization(args, train_set, val_set, test_set)\n",
    "else:\n",
    "    print('Use the manually specified hyperparameters')\n",
    "    exp_config = get_configure(args['model'])\n",
    "    main(args, exp_config, train_set, val_set, test_set)\n",
    "    trial_path = args['result_path'] + '/1'\n",
    "\n",
    "# Copy final\n",
    "copyfile(trial_path + '/model.pth', args['result_path'] + '/model.pth')\n",
    "copyfile(trial_path + '/configure.json', args['result_path'] + '/configure.json')\n",
    "copyfile(trial_path + '/eval.txt', args['result_path'] + '/eval.txt')\n",
    "\n",
    "with open(args['result_path']+'/configure.json', 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best hyper file: '+ trial_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWithHyper (args, exp_config, train_set, val_set, test_set):\n",
    "\n",
    "    # Record settings\n",
    "    exp_config.update({\n",
    "        'model': args['model'],\n",
    "        'n_tasks': args['n_tasks'],\n",
    "        'atom_featurizer_type': args['atom_featurizer_type'],\n",
    "        'bond_featurizer_type': args['bond_featurizer_type'],\n",
    "#         'patience': patienceNum,\n",
    "#         'batch_size':batch_size\n",
    "    })\n",
    "    if args['atom_featurizer_type'] != 'pre_train':\n",
    "        exp_config['in_node_feats'] = args['node_featurizer'].feat_size()\n",
    "    if args['edge_featurizer'] is not None and args['bond_featurizer_type'] != 'pre_train':\n",
    "        exp_config['in_edge_feats'] = args['edge_featurizer'].feat_size()\n",
    "\n",
    "    # Set up directory for saving results\n",
    "#     args = init_trial_path(args)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=exp_config['batch_size'], shuffle=True,\n",
    "                              collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=exp_config['batch_size'],\n",
    "                            collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    test_loader = DataLoader(dataset=test_set, batch_size=exp_config['batch_size'],\n",
    "                             collate_fn=collate_molgraphs, num_workers=args['num_workers'])\n",
    "    model = load_model(exp_config).to(args['device'])\n",
    "    \n",
    "    best_model_file = args['result_path']+'/saved_model/%s_bst_%s.pth' % (args['model'], split)\n",
    "\n",
    "    loss_criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    optimizer = Adam(model.parameters(), lr=exp_config['lr'],\n",
    "                     weight_decay=exp_config['weight_decay'])\n",
    "    stopper = EarlyStopping(patience=exp_config['patience'],\n",
    "                            filename=best_model_file,\n",
    "                            metric=args['metric'])\n",
    "    for epoch in range(args['num_epochs']):\n",
    "        # Train\n",
    "        run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer)\n",
    "\n",
    "        # Validation and early stop\n",
    "        val_score = run_an_eval_epoch(args, model, val_loader)\n",
    "        early_stop = stopper.step(val_score['roc_auc_score'], model)\n",
    "\n",
    "        if early_stop:\n",
    "            break\n",
    "\n",
    "    stopper.load_checkpoint(model)\n",
    "    \n",
    "    tr_scores = run_an_eval_epoch(args, model, train_loader)\n",
    "    val_scores = run_an_eval_epoch(args, model, val_loader)\n",
    "    te_scores = run_an_eval_epoch(args, model, test_loader)\n",
    "    \n",
    "    return tr_scores,val_scores,te_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_res = []\n",
    "val_res = []\n",
    "te_res = []\n",
    "\n",
    "for split in range(1, repetitions + 1):\n",
    "    \n",
    "    if args['augmentation']:\n",
    "        train_set, val_set, test_set = split_dataset_augmentation(df, split,args)\n",
    "        args['n_tasks'] = train_set.n_tasks\n",
    "    else:\n",
    "        train_set, val_set, test_set = split_dataset(args,dataset,split)\n",
    "        args['n_tasks'] = dataset.n_tasks\n",
    "\n",
    "    # Whether to take the logarithm of labels for narrowing the range of values\n",
    "    if args['log_values']:\n",
    "        train_set.labels = train_set.labels.log()\n",
    "        val_set.labels = val_set.labels.log()\n",
    "        test_set.labels = test_set.labels.log()\n",
    "    print('n_tasks : '+ str(args['n_tasks']))\n",
    "\n",
    "    tr_scores,val_scores,te_scores = trainWithHyper(args, config, train_set, val_set, test_set)\n",
    "\n",
    "    tr_res.append(tr_scores);\n",
    "    val_res.append(val_scores);\n",
    "    te_res.append(te_scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getList(res):\n",
    "    auc_prc_list = []\n",
    "    acc_list = []\n",
    "    auc_roc_list = []\n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    f1_list = []\n",
    "    kappa_list = []\n",
    "    mcc_list = []\n",
    "    for item in res:\n",
    "        auc_prc_list.append(item['all_score']['auc_prc'])\n",
    "        acc_list.append(item['all_score']['acc'])\n",
    "        auc_roc_list.append(item['all_score']['auc_roc'])\n",
    "        recall_list.append(item['all_score']['recall'])\n",
    "        precision_list.append(item['all_score']['precision'])\n",
    "        f1_list.append(item['all_score']['f1'])\n",
    "        kappa_list.append(item['all_score']['kappa'])\n",
    "        mcc_list.append(item['all_score']['mcc'])\n",
    "    return auc_prc_list,acc_list,auc_roc_list,recall_list,precision_list,f1_list,kappa_list,mcc_list\n",
    "\n",
    "tr_auc_prc_list,tr_acc_list,tr_auc_roc_list,tr_recall_list,tr_precision_list,tr_f1_list,tr_kappa_list,tr_mcc_list = getList(tr_res)\n",
    "val_auc_prc_list,val_acc_list,val_auc_roc_list,val_recall_list,val_precision_list,val_f1_list,val_kappa_list,val_mcc_list = getList(val_res)\n",
    "te_auc_prc_list,te_acc_list,te_auc_roc_list,te_recall_list,te_precision_list,te_f1_list,te_kappa_list,te_mcc_list = getList(te_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc auc_roc recall precision f1 kappa mcc\n",
    "acc_str = 'acc of training set is {:.3f}Â±{:.3f}, validation set is {:.3f}Â±{:.3f}, test set is {:.3f}Â±{:.3f}'.format(\n",
    "                np.mean(tr_acc_list), \n",
    "                np.std(tr_acc_list),\n",
    "                np.mean(val_acc_list), \n",
    "                np.std(val_acc_list),\n",
    "                np.mean(te_acc_list), \n",
    "                np.std(te_acc_list),\n",
    ")\n",
    "auc_str = 'auc_roc of training set is {:.3f}Â±{:.3f}, validation set is {:.3f}Â±{:.3f}, test set is {:.3f}Â±{:.3f}'.format(\n",
    "                np.mean(tr_auc_roc_list), \n",
    "                np.std(tr_auc_roc_list),\n",
    "                np.mean(val_auc_roc_list), \n",
    "                np.std(val_auc_roc_list),\n",
    "                np.mean(te_auc_roc_list), \n",
    "                np.std(te_auc_roc_list),\n",
    ")\n",
    "recall_str = 'recall of training set is {:.3f}Â±{:.3f}, validation set is {:.3f}Â±{:.3f}, test set is {:.3f}Â±{:.3f}'.format(\n",
    "                np.mean(tr_recall_list), \n",
    "                np.std(tr_recall_list),\n",
    "                np.mean(val_recall_list), \n",
    "                np.std(val_recall_list),\n",
    "                np.mean(te_recall_list), \n",
    "                np.std(te_recall_list),\n",
    ")\n",
    "precision_str = 'precision of training set is {:.3f}Â±{:.3f}, validation set is {:.3f}Â±{:.3f}, test set is {:.3f}Â±{:.3f}'.format(\n",
    "                np.mean(tr_precision_list), \n",
    "                np.std(tr_precision_list),\n",
    "                np.mean(val_precision_list), \n",
    "                np.std(val_precision_list),\n",
    "                np.mean(te_precision_list), \n",
    "                np.std(te_precision_list),\n",
    ")\n",
    "f1_str = 'f1 of training set is {:.3f}Â±{:.3f}, validation set is {:.3f}Â±{:.3f}, test set is {:.3f}Â±{:.3f}'.format(\n",
    "                np.mean(tr_f1_list), \n",
    "                np.std(tr_f1_list),\n",
    "                np.mean(val_f1_list), \n",
    "                np.std(val_f1_list),\n",
    "                np.mean(te_f1_list), \n",
    "                np.std(te_f1_list),\n",
    ")\n",
    "kappa_str = 'kappa of training set is {:.3f}Â±{:.3f}, validation set is {:.3f}Â±{:.3f}, test set is {:.3f}Â±{:.3f}'.format(\n",
    "                np.mean(tr_kappa_list), \n",
    "                np.std(tr_kappa_list),\n",
    "                np.mean(val_kappa_list), \n",
    "                np.std(val_kappa_list),\n",
    "                np.mean(te_kappa_list), \n",
    "                np.std(te_kappa_list),\n",
    ")\n",
    "mcc_str = 'mcc of training set is {:.3f}Â±{:.3f}, validation set is {:.3f}Â±{:.3f}, test set is {:.3f}Â±{:.3f}'.format(\n",
    "                np.mean(tr_mcc_list), \n",
    "                np.std(tr_mcc_list),\n",
    "                np.mean(val_mcc_list), \n",
    "                np.std(val_mcc_list),\n",
    "                np.mean(te_mcc_list), \n",
    "                np.std(te_mcc_list),\n",
    ")\n",
    "auc_prc_str = 'auc_prc of training set is {:.3f}Â±{:.3f}, validation set is {:.3f}Â±{:.3f}, test set is {:.3f}Â±{:.3f}'.format(\n",
    "                np.mean(tr_auc_prc_list), \n",
    "                np.std(tr_auc_prc_list),\n",
    "                np.mean(val_auc_prc_list), \n",
    "                np.std(val_auc_prc_list),\n",
    "                np.mean(te_auc_prc_list), \n",
    "                np.std(te_auc_prc_list),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc_str)\n",
    "print(auc_str)\n",
    "print(recall_str) \n",
    "print(precision_str)\n",
    "print(f1_str)\n",
    "print(kappa_str)\n",
    "print(mcc_str)\n",
    "print(auc_prc_str)\n",
    "print(args['model'])\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_arr = time.localtime(start_time)\n",
    "start_time_Style = time.strftime('%Y.%m.%d %H:%M:%S',start_time_arr)\n",
    "print('start_time', start_time_Style)\n",
    "end_time_arr = time.localtime(end_time)\n",
    "end_time_Style = time.strftime('%Y.%m.%d %H:%M:%S',end_time_arr)\n",
    "print('end_time', end_time_Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "dict1 = {\"model: \"+args['model']:['acc','auc_roc','recall','precision','f1','kappa','mcc','auc_prc'],\n",
    "         \"Train\":[np.mean(tr_acc_list),np.mean(tr_auc_roc_list),np.mean(tr_recall_list),np.mean(tr_precision_list), \n",
    "                  np.mean(tr_f1_list),np.mean(tr_kappa_list), np.mean(tr_mcc_list),np.mean(tr_auc_prc_list)],\n",
    "         \"Tr_STD\":[np.std(tr_acc_list),np.std(tr_auc_roc_list),np.std(tr_recall_list),np.std(tr_precision_list), \n",
    "                  np.std(tr_f1_list),np.std(tr_kappa_list), np.std(tr_mcc_list),np.std(tr_auc_prc_list)],\n",
    "         \"Validation\":[np.mean(val_acc_list),np.mean(val_auc_roc_list),np.mean(val_recall_list),np.mean(val_precision_list), \n",
    "                  np.mean(val_f1_list),np.mean(val_kappa_list), np.mean(val_mcc_list),np.mean(val_auc_prc_list)],\n",
    "         \"Va_STD\":[np.std(val_acc_list),np.std(val_auc_roc_list),np.std(val_recall_list),np.std(val_precision_list), \n",
    "                  np.std(val_f1_list),np.std(val_kappa_list), np.std(val_mcc_list),np.std(val_auc_prc_list)],\n",
    "         \"Test\":[np.mean(te_acc_list),np.mean(te_auc_roc_list),np.mean(te_recall_list),np.mean(te_precision_list), \n",
    "                  np.mean(te_f1_list),np.mean(te_kappa_list), np.mean(te_mcc_list),np.mean(te_auc_prc_list)],\n",
    "          \"Te_STD\":[np.std(te_acc_list),np.std(te_auc_roc_list),np.std(te_recall_list),np.std(te_precision_list), \n",
    "                  np.std(te_f1_list),np.std(te_kappa_list), np.std(te_mcc_list),np.std(te_auc_prc_list)]}\n",
    "dict1 = collections.OrderedDict(dict1)\n",
    "df = pd.DataFrame(dict1,index = None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(args['result_path'] + '/output.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "651aa032da6733035ad22364a32bbe31d47409d2b7e0323579a2c1f1e8e54f33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
